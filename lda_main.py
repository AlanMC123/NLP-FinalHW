import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from gensim import corpora, models
from gensim.models.ldamulticore import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import nltk
import re
import os
import multiprocessing
from joblib import Parallel, delayed
from tqdm import tqdm
from nltk.stem import WordNetLemmatizer

try:
    nltk.download('punkt', quiet=True)
    # --- æ–°å¢ï¼šä¸‹è½½è¯å½¢è¿˜åŸæ‰€éœ€çš„èµ„æº ---
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except Exception:
    pass

lemmatizer = WordNetLemmatizer()

# ==========================================
# 1. æ ¸å¿ƒæ•°é…ç½®
# ==========================================
TOTAL_CORES = multiprocessing.cpu_count()
if TOTAL_CORES > 4:
    NUM_CORES = TOTAL_CORES - 4
else:
    NUM_CORES = max(1, TOTAL_CORES - 1)

plt.rcParams['font.sans-serif'] = ['SimHei'] 
plt.rcParams['axes.unicode_minus'] = False  

result_folder = 'lda_analysis'
os.makedirs(result_folder, exist_ok=True)

try:
    nltk.download('punkt', quiet=True)
except Exception:
    pass

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================
stopwords_path = 'stopwords/lda_stopwords.txt'
custom_stopwords = []
if os.path.exists(stopwords_path):
    with open(stopwords_path, 'r', encoding='utf-8') as f:
        custom_stopwords = [line.strip() for line in f if line.strip()]

nltk_stopwords = stopwords.words('english')
all_stopwords = set(nltk_stopwords + custom_stopwords)

def preprocess_text(text):
    if pd.isna(text) or text is None:
        return []
    
    # å®šä¹‰éœ€è¦ä¿ç•™åŸå§‹å¤§å°å†™çš„æ”¿å…šåç§°
    party_names = ['Conservative', 'Labour', 'Liberal-Democrat', 'Scottish-National-Party', 'Plaid-Cymru', 
                  'Labourco-operative', 'UUP', 'DUP', 'Green', 'Independent', 
                  'Social-Democratic-and-Labour-Party', 'Respect', 'UKIP', 'Alliance', 
                  'Independent-Conservative', 'Independent-Ulster-Unionist']
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    text = str(text)
    
    # ç§»é™¤éå­—æ¯å’Œç©ºæ ¼çš„å­—ç¬¦
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # åˆ†è¯å¹¶å¤„ç†æ¯ä¸ªå•è¯
    words = []
    for word in text.split():
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¿å…šåç§°ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        is_party = False
        for party in party_names:
            if word.lower() == party.lower():
                # ä¿ç•™åŸå§‹å¤§å°å†™
                words.append(party)
                is_party = True
                break
        
        # å¦‚æœä¸æ˜¯æ”¿å…šåç§°ï¼Œåˆ™è½¬æ¢ä¸ºå°å†™
        if not is_party:
            # 1. è½¬å°å†™
            w = word.lower()
            # 2. è¯å½¢è¿˜åŸ (åŠ¨è¯è¿˜åŸï¼Œä¾‹å¦‚ voted -> vote)
            w = lemmatizer.lemmatize(w, pos='v')
            # 3.å†æ¬¡è¿˜åŸ (åè¯è¿˜åŸï¼Œä¾‹å¦‚ parties -> party)
            w = lemmatizer.lemmatize(w, pos='n')
            
            words.append(w)
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    words = [word for word in words if word not in all_stopwords and len(word) >= 2]
    
    return words

def plot_word_frequency(data, party_col, processed_text_col, prefix, result_folder):
    if party_col not in data.columns: return
    valid_data = data[data[party_col].notna()]
    if valid_data.empty: return

    major_parties = valid_data[party_col].value_counts().head(5).index.tolist()
    
    def get_top_words(party, n=20):
        party_data = valid_data[valid_data[party_col] == party]
        party_texts = party_data[processed_text_col]
        if party_texts.empty: return pd.Series(dtype='int64')
        all_words = [word for text in party_texts for word in text]
        return pd.Series(all_words).value_counts().head(n)
    
    try:
        plt.figure(figsize=(20, 15))
        if len(major_parties) > 0:
            rows = (len(major_parties) + 1) // 2
            for i, party in enumerate(major_parties, 1):
                plt.subplot(rows, 2, i)
                word_freq = get_top_words(party, 15)
                if len(word_freq) > 0:
                    sns.barplot(x=word_freq.values, y=word_freq.index.astype(str), hue=word_freq.index.astype(str), palette='viridis', legend=False)
                    plt.title(f'{party} é«˜é¢‘è¯')
                    plt.xlabel('è¯é¢‘')
                    plt.yticks(fontsize=10)
            plt.tight_layout(pad=2.0)
            plt.savefig(f'{result_folder}/{prefix}_party_word_frequency.png', dpi=300)
            plt.close()
        
        # ä¿å­˜æ–‡æœ¬
        all_parties = valid_data[party_col].unique().tolist()
        word_freq_file = os.path.join(result_folder, f'{prefix}_all_party_word_freq.txt')
        with open(word_freq_file, 'w', encoding='utf-8') as f:
            for party in all_parties:
                word_freq = get_top_words(party, 20)
                f.write(f"\n{party} è¯é¢‘ç»Ÿè®¡:\n")
                for word, freq in word_freq.items():
                    f.write(f"{word}: {freq}\n")
    except Exception as e:
        print(f"è¯é¢‘ç»˜å›¾é”™è¯¯: {e}")

def extract_dominant_topic_safe(topic_dist):
    """ä»ä¸»é¢˜åˆ†å¸ƒåˆ—è¡¨ä¸­æå–æ¦‚ç‡æœ€å¤§çš„ä¸»é¢˜ID"""
    if not topic_dist or not isinstance(topic_dist, list):
        return -1
    try:
        best_topic = max(topic_dist, key=lambda x: x[1])
        return best_topic[0]
    except:
        return -1

# ==========================================
# æ ¸å¿ƒè®­ç»ƒå‡½æ•° (å·²ä¿®å¤)
# ==========================================
def train_combined_lda_and_analyze(data, combined_text_col, party_col, year_col, prefix, result_folder, num_topics=6):
    print(f"\n{'='*50}")
    print(f"æ­£åœ¨å¯¹ {prefix} è¯­æ–™åº“è®­ç»ƒ LDA æ¨¡å‹...")
    
    texts = data[combined_text_col].tolist()
    dictionary = corpora.Dictionary(texts)
    
    print(f"åŸå§‹è¯æ±‡è¡¨å¤§å°: {len(dictionary)}")
    
    # --- ä¿®æ”¹ 1: æåº¦æ”¾å®½è¿‡æ»¤æ¡ä»¶ï¼Œæˆ–è€…ç›´æ¥æ³¨é‡Šæ‰ ---
    # ä¹‹å‰å¯èƒ½ no_below=5 åˆ é™¤äº†å¤ªå¤šè¯ï¼Œç°åœ¨æ”¹ä¸º 1 æˆ–è€… 2
    # no_above=0.9 è¡¨ç¤ºé™¤é 90% çš„æ–‡æ¡£éƒ½æœ‰è¿™ä¸ªè¯ï¼Œå¦åˆ™ä¿ç•™
    dictionary.filter_extremes(no_below=2, no_above=0.9) 
    print(f"è¿‡æ»¤åè¯æ±‡è¡¨å¤§å°: {len(dictionary)}")
    
    if len(dictionary) == 0:
        print("âŒ é”™è¯¯ï¼šè¯æ±‡è¡¨ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚")
        return data, None, None, None

    # æ„å»ºè¯­æ–™åº“
    corpus = [dictionary.doc2bow(text) for text in texts]
    
    # æ£€æŸ¥è¯­æ–™åº“æ˜¯å¦å…¨æ˜¯ç©ºçš„ (Debug)
    empty_docs = sum(1 for doc in corpus if not doc)
    print(f"ç©ºæ–‡æ¡£å‘é‡æ•°é‡: {empty_docs} / {len(corpus)}")
    if empty_docs == len(corpus):
        print("âŒ é”™è¯¯ï¼šæ‰€æœ‰æ–‡æ¡£çš„è¯è¢‹å‘é‡éƒ½ä¸ºç©ºï¼è¿™æ„å‘³ç€é¢„å¤„ç†åçš„è¯éƒ½ä¸åœ¨è¯å…¸é‡Œã€‚")
        return data, None, None, None

    print(f"æ­£åœ¨è®­ç»ƒ LDA æ¨¡å‹ (ä¸»é¢˜æ•°: {num_topics}, æ ¸å¿ƒæ•°: {NUM_CORES})...")
    lda_model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        random_state=42,
        chunksize=1500,
        passes=15,
        eta=0.001,
        alpha='asymmetric',
        per_word_topics=True,
        workers=NUM_CORES
    )
    
    # ä¿å­˜ä¸»é¢˜
    topic_file_path = os.path.join(result_folder, f'{prefix}_topic_results.txt')
    with open(topic_file_path, 'w', encoding='utf-8') as f:
        for idx, topic in lda_model.print_topics(-1):
            f.write(f"ä¸»é¢˜ {idx}: {topic}\n")

    # ==========================================
    # ä¿®å¤é‡ç‚¹ï¼šè®¡ç®—åˆ†å¸ƒ (å¼ºåˆ¶è¾“å‡ºæ¦‚ç‡)
    # ==========================================
    print(f"\næ­£åœ¨è®¡ç®—æ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ...")
    
    # --- ä¿®æ”¹ 2: ä½¿ç”¨ get_document_topics å¹¶è®¾ç½® minimum_probability=0 ---
    # è¿™æ ·å³ä½¿æ¦‚ç‡å¾ˆä½ï¼Œä¹Ÿä¸ä¼šè¿”å›ç©ºåˆ—è¡¨
    topic_distributions = []
    # è¿™é‡Œä¸èƒ½ç”¨å¤šè¿›ç¨‹ï¼Œå› ä¸º lda_model ä¼ é€’å¼€é”€å¤§ï¼Œç›´æ¥å•çº¿ç¨‹è·‘å¾ªç¯ï¼Œé€Ÿåº¦é€šå¸¸å¯ä»¥æ¥å—
    for i in tqdm(range(len(corpus)), desc="æ¨æ–­ä¸»é¢˜", ncols=80, unit="doc"):
        # minimum_probability=0 å¼ºåˆ¶è¿”å›å®Œæ•´åˆ†å¸ƒ
        dist = lda_model.get_document_topics(corpus[i], minimum_probability=0)
        topic_distributions.append(dist)
    
    data[f'{prefix}_topic_distribution'] = pd.Series(topic_distributions, index=data.index)
    
    print("æ­£åœ¨æå–ä¸»è¦ä¸»é¢˜...")
    data[f'{prefix}_dominant_topic'] = data[f'{prefix}_topic_distribution'].map(extract_dominant_topic_safe)
    
    # å†æ¬¡æ£€æŸ¥
    valid_topics_count = data[f'{prefix}_dominant_topic'].value_counts()
    print(f"ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡ (æ£€æŸ¥ç‚¹): \n{valid_topics_count.head()}")
    
    if -1 in valid_topics_count.index and valid_topics_count[-1] == len(data):
        print("âŒ ä¾ç„¶æœªèƒ½æå–åˆ°æœ‰æ•ˆä¸»é¢˜ã€‚è¯·æ£€æŸ¥æ•°æ®è´¨é‡ã€‚")
        return data, lda_model, dictionary, corpus

    # ==========================================
    # å¯è§†åŒ–
    # ==========================================
    print(f">>> å¼€å§‹ {party_col} åˆ†å¸ƒåˆ†æ <<<")
    
    analysis_data = data[
        data[party_col].notna() & 
        (data[party_col] != '') & 
        (data[f'{prefix}_dominant_topic'] != -1) 
    ].copy()
    
    print(f"å‚ä¸ç»˜å›¾çš„æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(analysis_data)}")
    
    if not analysis_data.empty:
        try:
            party_topic_dist = analysis_data.groupby(party_col)[f'{prefix}_dominant_topic'].value_counts(normalize=True).unstack(fill_value=0)
            
            plt.figure(figsize=(12, 8))
            party_topic_dist.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
            plt.title(f'å„å…šæ´¾ä¸»é¢˜åˆ†å¸ƒ')
            plt.xlabel('å…šæ´¾')
            plt.ylabel('æ¯”ä¾‹')
            plt.legend(title='ä¸»é¢˜', loc='upper right', bbox_to_anchor=(1.15, 1))
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_party_topic_distribution.png')
            plt.close()
            print("âœ… æŸ±çŠ¶å›¾å·²ä¿å­˜")
            
            plt.figure(figsize=(12, 10))
            sns.heatmap(party_topic_dist, annot=True, cmap='YlGnBu', fmt='.2f')
            plt.title(f'ä¸»é¢˜-å…šæ´¾åˆ†å¸ƒçƒ­åŠ›å›¾')
            plt.xlabel('ä¸»é¢˜')
            plt.ylabel('å…šæ´¾')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_topic_party_heatmap.png')
            plt.close()
            print("âœ… çƒ­åŠ›å›¾å·²ä¿å­˜")
            
        except Exception as e:
            print(f"âŒ ç»˜å›¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # å¹´ä»½ä¸»é¢˜åˆ†æ
    print(f"\n>>> å¼€å§‹ {year_col} åˆ†å¸ƒåˆ†æ <<<")
    
    year_analysis_data = data[
        data[year_col].notna() & 
        (data[f'{prefix}_dominant_topic'] != -1) 
    ].copy()
    
    print(f"å‚ä¸å¹´ä»½åˆ†æçš„æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(year_analysis_data)}")
    
    if not year_analysis_data.empty:
        try:
            # è®¡ç®—æ¯å¹´çš„ä¸»é¢˜åˆ†å¸ƒ
            year_topic_counts = year_analysis_data.groupby(year_col)[f'{prefix}_dominant_topic'].value_counts().unstack(fill_value=0)
            year_topic_dist = year_analysis_data.groupby(year_col)[f'{prefix}_dominant_topic'].value_counts(normalize=True).unstack(fill_value=0)
            
            # æ‰¾å‡ºæ¯å¹´çš„æœ€å¤§ä¸»é¢˜
            annual_max_topics = year_topic_counts.idxmax(axis=1)
            annual_max_proportions = year_topic_counts.max(axis=1) / year_topic_counts.sum(axis=1)
            
            # ä¿å­˜æ¯å¹´æœ€å¤§ä¸»é¢˜åˆ°æ–‡ä»¶
            year_max_topic_file = os.path.join(result_folder, f'{prefix}_annual_max_topics.txt')
            with open(year_max_topic_file, 'w', encoding='utf-8') as f:
                f.write("å¹´åº¦æœ€å¤§ä¸»é¢˜åˆ†æ\n")
                f.write("=" * 30 + "\n")
                f.write(f"{'å¹´ä»½':<8} {'æœ€å¤§ä¸»é¢˜':<10} {'ä¸»é¢˜æ¯”ä¾‹':<10} {'æ€»æ–‡æ¡£æ•°':<10}\n")
                f.write("-" * 40 + "\n")
                
                for year in sorted(annual_max_topics.index):
                    max_topic = annual_max_topics[year]
                    proportion = annual_max_proportions[year]
                    total_docs = year_topic_counts.loc[year].sum()
                    f.write(f"{year:<8} {max_topic:<10} {proportion:.2%}{'':<10} {total_docs:<10}\n")
            
            print("âœ… å¹´åº¦æœ€å¤§ä¸»é¢˜åˆ†æå·²ä¿å­˜")
            
            # å¯è§†åŒ–å¹´ä»½-ä¸»é¢˜åˆ†å¸ƒ
            plt.figure(figsize=(14, 8))
            year_topic_dist.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
            plt.title(f'æ¯å¹´ä¸»é¢˜åˆ†å¸ƒ')
            plt.xlabel('å¹´ä»½')
            plt.ylabel('æ¯”ä¾‹')
            plt.legend(title='ä¸»é¢˜', loc='upper right', bbox_to_anchor=(1.15, 1))
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_year_topic_distribution.png')
            plt.close()
            print("âœ… å¹´ä»½-ä¸»é¢˜åˆ†å¸ƒæŸ±çŠ¶å›¾å·²ä¿å­˜")
            
            plt.figure(figsize=(14, 10))
            sns.heatmap(year_topic_dist, annot=True, cmap='YlGnBu', fmt='.2f')
            plt.title(f'ä¸»é¢˜-å¹´ä»½åˆ†å¸ƒçƒ­åŠ›å›¾')
            plt.xlabel('ä¸»é¢˜')
            plt.ylabel('å¹´ä»½')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_topic_year_heatmap.png')
            plt.close()
            print("âœ… ä¸»é¢˜-å¹´ä»½åˆ†å¸ƒçƒ­åŠ›å›¾å·²ä¿å­˜")
            
            # å¯è§†åŒ–æ¯å¹´æœ€å¤§ä¸»é¢˜
            plt.figure(figsize=(12, 6))
            years = sorted(annual_max_topics.index)
            max_topics = [annual_max_topics[year] for year in years]
            max_props = [annual_max_proportions[year] for year in years]
            
            plt.bar(years, max_props, color='skyblue')
            for i, (year, topic) in enumerate(zip(years, max_topics)):
                plt.text(year, max_props[i] + 0.01, f'T{topic}', ha='center', fontsize=9)
            
            plt.title('æ¯å¹´æœ€å¤§ä¸»é¢˜æ¯”ä¾‹')
            plt.xlabel('å¹´ä»½')
            plt.ylabel('æœ€å¤§ä¸»é¢˜æ¯”ä¾‹')
            plt.ylim(0, 1)
            plt.xticks(years, rotation=45)
            plt.grid(axis='y', linestyle='--', alpha=0.7)
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_annual_max_topic_trend.png')
            plt.close()
            print("âœ… å¹´åº¦æœ€å¤§ä¸»é¢˜è¶‹åŠ¿å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"âŒ å¹´ä»½åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºå¹´ä»½åˆ†æã€‚")

    print("\næ­£åœ¨è¿›è¡Œè¯é¢‘ç»Ÿè®¡...")
    plot_word_frequency(data, party_col, combined_text_col, prefix, result_folder)
    
    # ==========================================
    # è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    # ==========================================
    print("\n>>> è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ <<<")
    
    # 1. å›°æƒ‘åº¦ (Perplexity)
    perplexity = lda_model.log_perplexity(corpus)
    print(f"\nğŸ” å›°æƒ‘åº¦ (Perplexity): {perplexity:.4f}")
    
    # 2. ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score) - ä½¿ç”¨å…¨é‡æ•°æ®å’Œå¤šçº¿ç¨‹
    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v', processes=NUM_CORES)
    coherence_score = coherence_model.get_coherence()
    print(f"ğŸ” ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score): {coherence_score:.4f}")
    
    # 3. ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity)
    def calculate_topic_diversity(model, top_n=10):
        """è®¡ç®—ä¸»é¢˜å¤šæ ·æ€§ï¼šä¸åŒä¸»é¢˜ä¸­å”¯ä¸€è¯çš„æ¯”ä¾‹"""
        topics = model.print_topics(num_words=top_n)
        all_words = set()
        total_words = 0
        
        for topic in topics:
            # æå–ä¸»é¢˜è¯
            topic_words = re.findall(r'"(\w+)"', topic[1])
            all_words.update(topic_words)
            total_words += len(topic_words)
        
        if total_words == 0:
            return 0.0
        
        return len(all_words) / total_words
    
    topic_diversity = calculate_topic_diversity(lda_model)
    print(f"ğŸ” ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity): {topic_diversity:.4f}")
    
    # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = os.path.join(result_folder, f'{prefix}_model_metrics.txt')
    with open(metrics_file, 'w', encoding='utf-8') as f:
        f.write("LDA æ¨¡å‹è¯„ä¼°æŒ‡æ ‡\n")
        f.write("=" * 30 + "\n")
        f.write(f"å›°æƒ‘åº¦ (Perplexity): {perplexity:.4f}\n")
        f.write(f"ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score): {coherence_score:.4f}\n")
        f.write(f"ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity): {topic_diversity:.4f}\n")
    print(f"âœ… æ¨¡å‹æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = 'models/lda'
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, 'lda.model')
    lda_model.save(model_path)
    print(f"âœ… LDA æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    return data, lda_model, dictionary, corpus

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == '__main__':
    print(f"ç‰©ç†æ ¸å¿ƒ: {TOTAL_CORES}, ä½¿ç”¨æ ¸å¿ƒ: {NUM_CORES}")
    csv_path = 'corpus/ParlVote_concat.csv'
    
    if os.path.exists(csv_path):
        print("æ­£åœ¨è¯»å–æ•°æ®...")
        df = pd.read_csv(csv_path, usecols=['debate_id', 'motion_party', 'debate_title', 'motion_text', 'party', 'speech'])
        
        # é¢„å¤„ç†
        print("é¢„å¤„ç† Motion...")
        motion_texts = df['motion_text'].tolist()
        df['processed_motion'] = Parallel(n_jobs=NUM_CORES)(delayed(preprocess_text)(t) for t in tqdm(motion_texts, ncols=80, unit="doc"))
        
        print("é¢„å¤„ç† Speech...")
        speech_texts = df['speech'].tolist()
        df['processed_speech'] = Parallel(n_jobs=NUM_CORES)(delayed(preprocess_text)(t) for t in tqdm(speech_texts, ncols=80, unit="doc"))
        
        # åˆå¹¶
        print("åˆå¹¶è¯­æ–™...")
        combined = [m + s for m, s in zip(df['processed_motion'], df['processed_speech'])]
        df['combined_text'] = combined
        
        # ä» debate_id æå–å¹´ä»½
        df['year'] = df['debate_id'].astype(str).str[:4].astype(int)
        print(f"å¹´ä»½èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
        
        # è¿‡æ»¤
        df_combined = df[df['combined_text'].apply(len) > 0].copy()
        print(f"è¿‡æ»¤åæ•°æ®é‡: {len(df_combined)}")

        # è®­ç»ƒ
        train_combined_lda_and_analyze(
            df_combined, 
            combined_text_col='combined_text', 
            party_col='motion_party', 
            year_col='year',  
            prefix='lda',  
            result_folder=result_folder,
            num_topics=8
        )
        
        print("\nç¨‹åºå®Œæˆï¼")
    else:
        print(f"æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")