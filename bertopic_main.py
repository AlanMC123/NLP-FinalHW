import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from gensim.models.coherencemodel import CoherenceModel
from gensim import corpora
import nltk
import re
import os
import multiprocessing
from joblib import Parallel, delayed
from tqdm import tqdm
from nltk.stem import WordNetLemmatizer
import warnings

warnings.filterwarnings('ignore')

try:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except Exception:
    pass

lemmatizer = WordNetLemmatizer()

# ==========================================
# 1. æ ¸å¿ƒæ•°é…ç½®
# ==========================================
TOTAL_CORES = multiprocessing.cpu_count()
if TOTAL_CORES > 4:
    NUM_CORES = TOTAL_CORES - 4
else:
    NUM_CORES = max(1, TOTAL_CORES - 1)
NUM_CORES = 10
CLUSTER_NUM = None   # æŒ‡å®šèšç±»æ•°é‡ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨èšç±»

plt.rcParams['font.sans-serif'] = ['SimHei'] 
plt.rcParams['axes.unicode_minus'] = False  

result_folder = f'bertopic_analysis_{CLUSTER_NUM}'
os.makedirs(result_folder, exist_ok=True)

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================
# ä½¿ç”¨ bert_stopwords.txt ä½œä¸ºä¸»è¦åœç”¨è¯è¡¨
stopwords_path = 'stopwords/bert_stopwords.txt'
all_stopwords = []
if os.path.exists(stopwords_path):
    with open(stopwords_path, 'r', encoding='utf-8') as f:
        all_stopwords = [line.strip() for line in f if line.strip()]
    all_stopwords = set(all_stopwords)
else:
    # å¦‚æœ bert_stopwords.txt ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çš„ nltk åœç”¨è¯
    nltk_stopwords = nltk.corpus.stopwords.words('english')
    all_stopwords = set(nltk_stopwords)

def preprocess_text(text):
    if pd.isna(text) or text is None:
        return ""
    
    # å®šä¹‰éœ€è¦ä¿ç•™åŸå§‹å¤§å°å†™çš„æ”¿å…šåç§°
    party_names = ['Conservative', 'Labour', 'Liberal-Democrat', 'Scottish-National-Party', 'Plaid-Cymru', 
                  'Labourco-operative', 'UUP', 'DUP', 'Green', 'Independent', 
                  'Social-Democratic-and-Labour-Party', 'Respect', 'UKIP', 'Alliance', 
                  'Independent-Conservative', 'Independent-Ulster-Unionist']
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    text = str(text)
    
    # ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # åˆ†è¯å¹¶å¤„ç†æ¯ä¸ªå•è¯
    words = []
    for word in text.split():
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¿å…šåç§°ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        is_party = False
        for party in party_names:
            if word == party.lower():
                # ä¿ç•™åŸå§‹å¤§å°å†™
                words.append(party)
                is_party = True
                break
        
        # å¦‚æœä¸æ˜¯æ”¿å…šåç§°
        if not is_party:
            # è¯å½¢è¿˜åŸ (åŠ¨è¯è¿˜åŸï¼Œä¾‹å¦‚ voted -> vote)
            w = lemmatizer.lemmatize(word, pos='v')
            # å†æ¬¡è¿˜åŸ (åè¯è¿˜åŸï¼Œä¾‹å¦‚ parties -> party)
            w = lemmatizer.lemmatize(w, pos='n')
            
            words.append(w)
    
    # ä¸¥æ ¼è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    filtered_words = []
    for word in words:
        # è¿‡æ»¤åœç”¨è¯
        if word in all_stopwords:
            continue
        # è¿‡æ»¤çŸ­è¯
        if len(word) < 3:
            continue
        # è¿‡æ»¤æ•°å­—è¯
        if word.isdigit():
            continue
        filtered_words.append(word)
    
    # è¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯åˆ—è¡¨ï¼Œå› ä¸º BERTopic éœ€è¦åŸå§‹æ–‡æœ¬æˆ–å¥å­åˆ—è¡¨
    return ' '.join(filtered_words)

def plot_word_frequency(data, party_col, processed_text_col, prefix, result_folder):
    if party_col not in data.columns: return
    valid_data = data[data[party_col].notna()]
    if valid_data.empty: return

    major_parties = valid_data[party_col].value_counts().head(5).index.tolist()
    
    def get_top_words(party, n=20):
        party_data = valid_data[valid_data[party_col] == party]
        party_texts = party_data[processed_text_col]
        if party_texts.empty: return pd.Series(dtype='int64')
        all_words = [word for text in party_texts for word in text.split()]
        return pd.Series(all_words).value_counts().head(n)
    
    try:
        plt.figure(figsize=(20, 15))
        if len(major_parties) > 0:
            rows = (len(major_parties) + 1) // 2
            for i, party in enumerate(major_parties, 1):
                plt.subplot(rows, 2, i)
                word_freq = get_top_words(party, 15)
                if len(word_freq) > 0:
                    sns.barplot(x=word_freq.values, y=word_freq.index.astype(str), hue=word_freq.index.astype(str), palette='viridis', legend=False)
                    plt.title(f'{party} é«˜é¢‘è¯')
                    plt.xlabel('è¯é¢‘')
                    plt.yticks(fontsize=10)
            plt.tight_layout(pad=2.0)
            plt.savefig(f'{result_folder}/{prefix}_party_word_frequency.png', dpi=300)
            plt.close()
        
        # ä¿å­˜æ–‡æœ¬
        all_parties = valid_data[party_col].unique().tolist()
        word_freq_file = os.path.join(result_folder, f'{prefix}_all_party_word_freq.txt')
        with open(word_freq_file, 'w', encoding='utf-8') as f:
            for party in all_parties:
                word_freq = get_top_words(party, 20)
                f.write(f"\n{party} è¯é¢‘ç»Ÿè®¡:\n")
                for word, freq in word_freq.items():
                    f.write(f"{word}: {freq}\n")
    except Exception as e:
        print(f"è¯é¢‘ç»˜å›¾é”™è¯¯: {e}")

# ==========================================
# æ ¸å¿ƒè®­ç»ƒå‡½æ•°
# ==========================================
def train_bertopic_and_analyze(data, text_col, party_col, year_col, prefix, result_folder, num_topics=8):
    print(f"\n{'='*50}")
    print(f"æ­£åœ¨å¯¹ {prefix} è¯­æ–™åº“è®­ç»ƒ BERTopic æ¨¡å‹...")
    
    texts = data[text_col].tolist()
    print(f"è¯­æ–™åº“å¤§å°: {len(texts)} æ–‡æ¡£")
    
    # åˆå§‹åŒ– BERTopic æ¨¡å‹ï¼Œè°ƒæ•´å‚æ•°ä»¥æé«˜ä¸»é¢˜è´¨é‡
    # å–æ¶ˆæŒ‡å®šèšç±»æ•°ï¼Œè®©æ¨¡å‹è‡ªè¡Œå†³å®šæœ€ä¼˜ä¸»é¢˜æ•°é‡ï¼ˆnr_topics=Noneï¼‰
    model = BERTopic(
        nr_topics=num_topics,       # è‡ªåŠ¨å†³å®šä¸»é¢˜æ•°é‡
        language='english',
        calculate_probabilities=True,
        verbose=True,
        min_topic_size=100,  # å¢åŠ æœ€å°ä¸»é¢˜å¤§å°ï¼Œè¿‡æ»¤å°ä¸»é¢˜
        top_n_words=10,       # åªæ˜¾ç¤ºå‰10ä¸ªå…³é”®è¯
        n_gram_range=(1, 1),  # ä¸è€ƒè™‘è¯ç»„
        low_memory=True       # å†…å­˜ä¼˜åŒ–
    )
    
    # è®­ç»ƒæ¨¡å‹
    topics, probabilities = model.fit_transform(texts)
    
    # å°†ä¸»é¢˜åˆ†é…æ·»åŠ åˆ°æ•°æ®ä¸­
    data[f'{prefix}_topic'] = topics
    data[f'{prefix}_topic_probability'] = [max(prob) if len(prob) > 0 else 0 for prob in probabilities]
    
    # ä¿å­˜ä¸»é¢˜ä¿¡æ¯
    topic_info = model.get_topic_info()
    print(f"ä¸»é¢˜ä¿¡æ¯:\n{topic_info}")
    
    # ä¿å­˜ä¸»é¢˜æè¿°
    topic_file_path = os.path.join(result_folder, f'{prefix}_topic_results.txt')
    with open(topic_file_path, 'w', encoding='utf-8') as f:
        f.write("BERTopic ä¸»é¢˜ç»“æœ\n")
        f.write("=" * 30 + "\n\n")
        for idx in topic_info.Topic:
            if idx != -1:  # æ’é™¤å¼‚å¸¸ä¸»é¢˜
                f.write(f"ä¸»é¢˜ {idx}:\n")
                topic_terms = model.get_topic(idx)
                for term, weight in topic_terms:
                    f.write(f"  {term}: {weight:.4f}\n")
                f.write("\n")
    
    # ä¿å­˜å¹´åº¦å’Œæ”¿å…šTop5è¯é¢˜çš„æ–‡ä»¶
    top_topics_file = os.path.join(result_folder, f'{prefix}_top_topics.txt')
    
    # ==========================================
    # å¯è§†åŒ–
    # ==========================================
    print(f">>> å¼€å§‹ {party_col} åˆ†å¸ƒåˆ†æ <<<")
    
    analysis_data = data[
        data[party_col].notna() & 
        (data[party_col] != '') & 
        (data[f'{prefix}_topic'] != -1) 
    ].copy()
    
    print(f"å‚ä¸ç»˜å›¾çš„æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(analysis_data)}")
    
    if not analysis_data.empty:
        try:
            party_topic_dist = analysis_data.groupby(party_col)[f'{prefix}_topic'].value_counts(normalize=True).unstack(fill_value=0)
            
            plt.figure(figsize=(12, 8))
            party_topic_dist.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
            plt.title(f'å„å…šæ´¾ä¸»é¢˜åˆ†å¸ƒ')
            plt.xlabel('å…šæ´¾')
            plt.ylabel('æ¯”ä¾‹')
            plt.legend(title='ä¸»é¢˜', loc='upper right', bbox_to_anchor=(1.15, 1))
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_party_topic_distribution.png')
            plt.close()
            print("âœ… æŸ±çŠ¶å›¾å·²ä¿å­˜")
            
            plt.figure(figsize=(12, 10))
            sns.heatmap(party_topic_dist, annot=True, cmap='YlGnBu', fmt='.2f')
            plt.title(f'ä¸»é¢˜-å…šæ´¾åˆ†å¸ƒçƒ­åŠ›å›¾')
            plt.xlabel('ä¸»é¢˜')
            plt.ylabel('å…šæ´¾')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_topic_party_heatmap.png')
            plt.close()
            print("âœ… çƒ­åŠ›å›¾å·²ä¿å­˜")
            
            # è¾“å‡ºæ¯ä¸ªæ”¿å…šçš„Top5è¯é¢˜
            print("\n>>> ç”Ÿæˆæ¯ä¸ªæ”¿å…šTop5è¯é¢˜ <<<")
            party_topics_file = os.path.join(result_folder, f'{prefix}_party_topics.txt')
            with open(party_topics_file, 'w', encoding='utf-8') as f:
                f.write("BERTopic æ”¿å…šTop5è¯é¢˜åˆ†æ\n")
                f.write("=" * 50 + "\n\n")
                
                # æ¯ä¸ªæ”¿å…šçš„Top5è¯é¢˜
                f.write("1. æ¯ä¸ªæ”¿å…šTop5è¯é¢˜\n")
                f.write("-" * 30 + "\n\n")
                
                # éå†æ¯ä¸ªæ”¿å…š
                for party in sorted(party_topic_dist.index):
                    # è·å–è¯¥æ”¿å…šçš„ä¸»é¢˜åˆ†å¸ƒï¼ŒæŒ‰æ¯”ä¾‹é™åºæ’åˆ—
                    party_topics = party_topic_dist.loc[party].sort_values(ascending=False)
                    top_5_topics = party_topics.head(5)
                    
                    f.write(f"æ”¿å…š {party}:\n")
                    for i, (topic_id, proportion) in enumerate(top_5_topics.items(), 1):
                        # è·å–ä¸»é¢˜å…³é”®è¯
                        topic_terms = model.get_topic(topic_id)
                        keywords = [term for term, _ in topic_terms[:5]]
                        f.write(f"  {i}. ä¸»é¢˜ {topic_id}: æ¯”ä¾‹={proportion:.2%}, å…³é”®è¯={', '.join(keywords)}\n")
                    f.write("\n")
            print(f"âœ… æ”¿å…šTop5è¯é¢˜å·²ä¿å­˜åˆ°: {party_topics_file}")
            
        except Exception as e:
            print(f"âŒ ç»˜å›¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # å¹´ä»½ä¸»é¢˜åˆ†æ
    print(f"\n>>> å¼€å§‹ {year_col} åˆ†å¸ƒåˆ†æ <<<")
    
    year_analysis_data = data[
        data[year_col].notna() & 
        (data[f'{prefix}_topic'] != -1) 
    ].copy()
    
    print(f"å‚ä¸å¹´ä»½åˆ†æçš„æœ‰æ•ˆæ•°æ®è¡Œæ•°: {len(year_analysis_data)}")
    
    if not year_analysis_data.empty:
        try:
            # è®¡ç®—æ¯å¹´çš„ä¸»é¢˜åˆ†å¸ƒ
            year_topic_counts = year_analysis_data.groupby(year_col)[f'{prefix}_topic'].value_counts().unstack(fill_value=0)
            year_topic_dist = year_analysis_data.groupby(year_col)[f'{prefix}_topic'].value_counts(normalize=True).unstack(fill_value=0)
            
            # æ‰¾å‡ºæ¯å¹´çš„æœ€å¤§ä¸»é¢˜
            annual_max_topics = year_topic_counts.idxmax(axis=1)
            annual_max_proportions = year_topic_counts.max(axis=1) / year_topic_counts.sum(axis=1)
            
            # ä¿å­˜æ¯å¹´æœ€å¤§ä¸»é¢˜åˆ°æ–‡ä»¶
            year_max_topic_file = os.path.join(result_folder, f'{prefix}_annual_max_topics.txt')
            with open(year_max_topic_file, 'w', encoding='utf-8') as f:
                f.write("å¹´åº¦æœ€å¤§ä¸»é¢˜åˆ†æ\n")
                f.write("=" * 30 + "\n")
                f.write(f"{'å¹´ä»½':<8} {'æœ€å¤§ä¸»é¢˜':<10} {'ä¸»é¢˜æ¯”ä¾‹':<10} {'æ€»æ–‡æ¡£æ•°':<10}\n")
                f.write("-" * 40 + "\n")
                
                for year in sorted(annual_max_topics.index):
                    max_topic = annual_max_topics[year]
                    proportion = annual_max_proportions[year]
                    total_docs = year_topic_counts.loc[year].sum()
                    f.write(f"{year:<8} {max_topic:<10} {proportion:.2%}{'':<10} {total_docs:<10}\n")
            
            print("âœ… å¹´åº¦æœ€å¤§ä¸»é¢˜åˆ†æå·²ä¿å­˜")
            
            # å¯è§†åŒ–å¹´ä»½-ä¸»é¢˜åˆ†å¸ƒ
            plt.figure(figsize=(14, 8))
            year_topic_dist.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
            plt.title(f'æ¯å¹´ä¸»é¢˜åˆ†å¸ƒ')
            plt.xlabel('å¹´ä»½')
            plt.ylabel('æ¯”ä¾‹')
            plt.legend(title='ä¸»é¢˜', loc='upper right', bbox_to_anchor=(1.15, 1))
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_year_topic_distribution.png')
            plt.close()
            print("âœ… å¹´ä»½-ä¸»é¢˜åˆ†å¸ƒæŸ±çŠ¶å›¾å·²ä¿å­˜")
            
            plt.figure(figsize=(14, 10))
            sns.heatmap(year_topic_dist, annot=True, cmap='YlGnBu', fmt='.2f')
            plt.title(f'ä¸»é¢˜-å¹´ä»½åˆ†å¸ƒçƒ­åŠ›å›¾')
            plt.xlabel('ä¸»é¢˜')
            plt.ylabel('å¹´ä»½')
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_topic_year_heatmap.png')
            plt.close()
            print("âœ… ä¸»é¢˜-å¹´ä»½åˆ†å¸ƒçƒ­åŠ›å›¾å·²ä¿å­˜")
            
            # å¯è§†åŒ–æ¯å¹´æœ€å¤§ä¸»é¢˜
            plt.figure(figsize=(12, 6))
            years = sorted(annual_max_topics.index)
            max_topics = [annual_max_topics[year] for year in years]
            max_props = [annual_max_proportions[year] for year in years]
            
            plt.bar(years, max_props, color='skyblue')
            for i, (year, topic) in enumerate(zip(years, max_topics)):
                plt.text(year, max_props[i] + 0.01, f'T{topic}', ha='center', fontsize=9)
            
            plt.title('æ¯å¹´æœ€å¤§ä¸»é¢˜æ¯”ä¾‹')
            plt.xlabel('å¹´ä»½')
            plt.ylabel('æœ€å¤§ä¸»é¢˜æ¯”ä¾‹')
            plt.ylim(0, 1)
            plt.xticks(years, rotation=45)
            plt.grid(axis='y', linestyle='--', alpha=0.7)
            plt.tight_layout()
            plt.savefig(f'{result_folder}/{prefix}_annual_max_topic_trend.png')
            plt.close()
            print("âœ… å¹´åº¦æœ€å¤§ä¸»é¢˜è¶‹åŠ¿å›¾å·²ä¿å­˜")
            
            # è¾“å‡ºæ¯å¹´çš„Top5è¯é¢˜
            print("\n>>> ç”Ÿæˆæ¯å¹´Top5è¯é¢˜ <<<")
            with open(top_topics_file, 'w', encoding='utf-8') as f:
                f.write("BERTopic å¹´åº¦ä¸æ”¿å…šTop5è¯é¢˜åˆ†æ\n")
                f.write("=" * 50 + "\n\n")
                
                # 1. æ¯å¹´çš„Top5è¯é¢˜
                f.write("1. æ¯å¹´Top5è¯é¢˜\n")
                f.write("-" * 30 + "\n\n")
                
                # éå†æ¯ä¸€å¹´
                for year in sorted(year_topic_counts.index):
                    # è·å–è¯¥å¹´çš„ä¸»é¢˜åˆ†å¸ƒï¼ŒæŒ‰æ•°é‡é™åºæ’åˆ—
                    year_topics = year_topic_counts.loc[year].sort_values(ascending=False)
                    top_5_topics = year_topics.head(5)
                    
                    f.write(f"å¹´ä»½ {year}:\n")
                    for i, (topic_id, count) in enumerate(top_5_topics.items(), 1):
                        proportion = year_topic_dist.loc[year, topic_id]
                        # è·å–ä¸»é¢˜å…³é”®è¯
                        topic_terms = model.get_topic(topic_id)
                        keywords = [term for term, _ in topic_terms[:5]]
                        f.write(f"  {i}. ä¸»é¢˜ {topic_id}: æ–‡æ¡£æ•°={count}, æ¯”ä¾‹={proportion:.2%}, å…³é”®è¯={', '.join(keywords)}\n")
                    f.write("\n")
            print("âœ… å¹´åº¦Top5è¯é¢˜å·²ä¿å­˜")
        except Exception as e:
            print(f"âŒ å¹´ä»½åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºå¹´ä»½åˆ†æã€‚")

    # BERTopic ç‰¹æœ‰çš„å¯è§†åŒ–
    print("\n>>> ç”Ÿæˆ BERTopic ç‰¹æœ‰å¯è§†åŒ– <<<")
    
    # ä¸»é¢˜è¯äº‘
    try:
        fig = model.visualize_topics()
        fig.write_html(os.path.join(result_folder, f'{prefix}_topic_visualization.html'))
        print("âœ… ä¸»é¢˜å¯è§†åŒ– HTML å·²ä¿å­˜")
    except Exception as e:
        print(f"âŒ ä¸»é¢˜å¯è§†åŒ–å¤±è´¥: {e}")
    
    # ä¸»é¢˜å±‚æ¬¡æ ‘
    try:
        fig = model.visualize_hierarchy()
        fig.write_html(os.path.join(result_folder, f'{prefix}_topic_hierarchy.html'))
        print("âœ… ä¸»é¢˜å±‚æ¬¡æ ‘ HTML å·²ä¿å­˜")
    except Exception as e:
        print(f"âŒ ä¸»é¢˜å±‚æ¬¡æ ‘å¤±è´¥: {e}")
    
    # ä¸»é¢˜ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
    try:
        fig = model.visualize_heatmap()
        fig.write_html(os.path.join(result_folder, f'{prefix}_topic_heatmap.html'))
        print("âœ… ä¸»é¢˜ç›¸ä¼¼åº¦çƒ­åŠ›å›¾ HTML å·²ä¿å­˜")
    except Exception as e:
        print(f"âŒ ä¸»é¢˜ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å¤±è´¥: {e}")
    
    # ==========================================
    # è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
    # ==========================================
    print("\n>>> è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ <<<")
    
    # 1. ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score)
    def calculate_bertopic_coherence(model, texts, top_n=10, coherence_method='c_v', workers=NUM_CORES):
        """ä¸º BERTopic æ¨¡å‹è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§ï¼Œå…¨é‡æ•°æ®å¤šçº¿ç¨‹ç‰ˆæœ¬"""
        # è·å–ä¸»é¢˜è¯åˆ—è¡¨
        topics = model.get_topics()
        topic_words = []
        for topic_id, words in topics.items():
            if topic_id != -1:  # æ’é™¤å¼‚å¸¸ä¸»é¢˜
                topic_words.append([word for word, _ in words[:top_n]])
        
        # ä¸ºä¸»é¢˜ä¸€è‡´æ€§è®¡ç®—å‡†å¤‡è¯­æ–™åº“
        tokenized_texts = [text.split() for text in texts]
        
        # åˆ›å»ºå­—å…¸
        dictionary = corpora.Dictionary(tokenized_texts)
        
        # æ ¹æ®æ–¹æ³•é€‰æ‹©ä¸åŒçš„è®¡ç®—æ–¹å¼
        if coherence_method == 'u_mass':
            # u_mass æ–¹æ³•éœ€è¦è¯­æ–™åº“
            corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
            coherence_model = CoherenceModel(topics=topic_words, corpus=corpus, dictionary=dictionary, coherence='u_mass', processes=workers)
        else:
            # c_v æˆ–å…¶ä»–æ–¹æ³•ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ
            coherence_model = CoherenceModel(topics=topic_words, texts=tokenized_texts, dictionary=dictionary, coherence=coherence_method, processes=workers)
        
        return coherence_model.get_coherence()
    
    # ä½¿ç”¨å…¨é‡æ•°æ®å’Œå¤šçº¿ç¨‹è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§
    coherence_score = calculate_bertopic_coherence(model, texts, coherence_method='c_v', workers=NUM_CORES)
    print(f"ğŸ” ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score, c_v): {coherence_score:.4f}")
    
    # 2. ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity)
    def calculate_topic_diversity(model, top_n=10):
        """è®¡ç®—ä¸»é¢˜å¤šæ ·æ€§ï¼šä¸åŒä¸»é¢˜ä¸­å”¯ä¸€è¯çš„æ¯”ä¾‹"""
        topics = model.get_topics()
        all_words = set()
        total_words = 0
        
        for topic_id, words in topics.items():
            if topic_id != -1:  # æ’é™¤å¼‚å¸¸ä¸»é¢˜
                topic_words = [word for word, _ in words[:top_n]]
                all_words.update(topic_words)
                total_words += len(topic_words)
        
        if total_words == 0:
            return 0.0
        
        return len(all_words) / total_words
    
    topic_diversity = calculate_topic_diversity(model)
    print(f"ğŸ” ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity): {topic_diversity:.4f}")
    
    # 3. å›°æƒ‘åº¦ (Perplexity) - BERTopic ç‰ˆæœ¬ï¼Œå…¨é‡æ•°æ®ç‰ˆæœ¬
    def calculate_bertopic_perplexity(probabilities):
        """åŸºäºæ¦‚ç‡åˆ†å¸ƒè®¡ç®— BERTopic æ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œä½¿ç”¨å…¨é‡æ•°æ®"""
        # è¿‡æ»¤æœ‰æ•ˆæ¦‚ç‡
        valid_probs = [prob for prob in probabilities if isinstance(prob, np.ndarray) and len(prob) > 0]
        
        if not valid_probs:
            return 0.0
        
        # ä½¿ç”¨å…¨é‡æ•°æ®ï¼Œå‘é‡åŒ–è®¡ç®—ç†µï¼Œæé«˜æ•ˆç‡
        valid_probs_array = np.array(valid_probs)
        entropy = -np.sum(valid_probs_array * np.log(valid_probs_array + 1e-12), axis=1)
        
        # å›°æƒ‘åº¦æ˜¯å¹³å‡ç†µçš„æŒ‡æ•°
        avg_entropy = np.mean(entropy)
        perplexity = np.exp(avg_entropy)
        return perplexity
    
    perplexity = calculate_bertopic_perplexity(probabilities)
    print(f"ğŸ” å›°æƒ‘åº¦ (Perplexity): {perplexity:.4f}")
    
    # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶
    metrics_file = os.path.join(result_folder, f'{prefix}_model_metrics.txt')
    with open(metrics_file, 'w', encoding='utf-8') as f:
        f.write("BERTopic æ¨¡å‹è¯„ä¼°æŒ‡æ ‡\n")
        f.write("=" * 30 + "\n")
        f.write(f"ä¸»é¢˜ä¸€è‡´æ€§ (Coherence Score): {coherence_score:.4f}\n")
        f.write(f"ä¸»é¢˜å¤šæ ·æ€§ (Topic Diversity): {topic_diversity:.4f}\n")
        f.write(f"å›°æƒ‘åº¦ (Perplexity): {perplexity:.4f}\n")
    print(f"âœ… æ¨¡å‹æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = 'models/'
    os.makedirs(model_dir, exist_ok=True)
    model.save(os.path.join(model_dir, 'bertopic_model'))
    print(f"âœ… BERTopic æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}/bertopic_model")
    
    return data, model

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
if __name__ == '__main__':
    print(f"ç‰©ç†æ ¸å¿ƒ: {TOTAL_CORES}, ä½¿ç”¨æ ¸å¿ƒ: {NUM_CORES}")
    csv_path = 'corpus/ParlVote_concat.csv'
    
    if os.path.exists(csv_path):
        print("æ­£åœ¨è¯»å–æ•°æ®...")
        df = pd.read_csv(csv_path, usecols=['debate_id', 'motion_party', 'debate_title', 'motion_text', 'party', 'speech'])
        
        # é¢„å¤„ç†
        print("é¢„å¤„ç† Motion...")
        motion_texts = df['motion_text'].tolist()
        df['processed_motion'] = Parallel(n_jobs=NUM_CORES)(delayed(preprocess_text)(t) for t in tqdm(motion_texts, ncols=80, unit="doc"))
        
        print("é¢„å¤„ç† Speech...")
        speech_texts = df['speech'].tolist()
        df['processed_speech'] = Parallel(n_jobs=NUM_CORES)(delayed(preprocess_text)(t) for t in tqdm(speech_texts, ncols=80, unit="doc"))
        
        # åˆå¹¶
        print("åˆå¹¶è¯­æ–™...")
        df['combined_text'] = df['processed_motion'] + ' ' + df['processed_speech']
        
        # ä» debate_id æå–å¹´ä»½
        df['year'] = df['debate_id'].astype(str).str[:4].astype(int)
        print(f"å¹´ä»½èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
        
        # è¿‡æ»¤
        df_filtered = df[df['combined_text'].apply(len) > 0].copy()
        print(f"è¿‡æ»¤åæ•°æ®é‡: {len(df_filtered)}")

        # è®­ç»ƒ
        train_bertopic_and_analyze(
            df_filtered, 
            text_col='combined_text', 
            party_col='motion_party', 
            year_col='year',  
            prefix='bertopic',  
            result_folder=result_folder,
            num_topics=CLUSTER_NUM,
        )
        
        print("\nç¨‹åºå®Œæˆï¼")
    else:
        print(f"æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")