import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import multiprocessing
import os
from time import time
from collections import Counter

# NLP & æœºå™¨å­¦ä¹ åº“
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score 
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

# å¹¶è¡Œå¤„ç†åº“
from joblib import Parallel, delayed
from tqdm import tqdm

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_context("notebook", font_scale=1.2)
sns.set_style("whitegrid")
plt.rcParams['axes.unicode_minus'] = False
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
except:
    pass

# ==========================================
# 0. è®¾å®šæ ¸å¿ƒæ•°
# ==========================================
NUM_CORES = max(1, multiprocessing.cpu_count() - 2)
print(f"â˜… å·²è®¾å®šä½¿ç”¨ {NUM_CORES} ä¸ªçº¿ç¨‹è¿›è¡Œå¹¶è¡ŒåŠ é€Ÿ")

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def get_wordnet_pos(tag):
    """
    å°†NLTKçš„POSæ ‡ç­¾è½¬æ¢ä¸ºWordNetçš„POSæ ‡ç­¾
    """
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # é»˜è®¤ä½¿ç”¨åè¯

def load_stopwords(filepath='stopwords/w2v_stopwords.txt'):
    stopwords = set()
    if os.path.exists(filepath):
        print(f"æ­£åœ¨åŠ è½½åœç”¨è¯è¡¨: {filepath} ...")
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                stopwords = set(line.strip() for line in f if line.strip())
            print(f"æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯ã€‚")
        except Exception as e:
            print(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
    return stopwords

# â˜… ä¿®æ”¹ï¼šæ·»åŠ  use_lemmatization å‚æ•°å¹¶å®ç°è¯å½¢è¿˜åŸé€»è¾‘
def preprocess_wrapper(text, stopwords_set=None, use_lemmatization=True):
    """
    é¢„å¤„ç†æµç¨‹ï¼šåˆ†è¯ -> å»åœç”¨è¯ -> è¯å½¢è¿˜åŸ
    """
    if pd.isna(text):
        return []
    
    # 1. åˆ†è¯ (Gensim çš„ simple_preprocess ä¼šè‡ªåŠ¨è½¬å°å†™å¹¶å»æ ‡ç‚¹)
    tokens = simple_preprocess(str(text))
    
    # 2. å…ˆå»åœç”¨è¯ (æ­¤æ—¶ token æ˜¯å®Œæ•´çš„ï¼Œèƒ½åŒ¹é…ä¸Š stopwords é‡Œçš„ 'government')
    if stopwords_set:
        tokens = [t for t in tokens if t not in stopwords_set]
    
    # 3. æœ€åå¯¹å‰©ä¸‹çš„è¯è¿›è¡Œè¯å½¢è¿˜åŸ
    if use_lemmatization:
        # åœ¨å‡½æ•°å†…åˆå§‹åŒ–ï¼Œé˜²æ­¢å¤šè¿›ç¨‹å†²çª
        lemmatizer = WordNetLemmatizer()
        # ç®€åŒ–è¯å½¢è¿˜åŸï¼ˆä¸ä½¿ç”¨POSæ ‡ç­¾ï¼Œé¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜ï¼‰
        tokens = [lemmatizer.lemmatize(t) for t in tokens]
        
    return tokens

def get_cluster_keywords(texts, stop_words_list=None, top_n=5):
    """
    æå–èšç±»å…³é”®è¯ (TF-IDF)
    è‡ªåŠ¨å¤„ç†åœç”¨è¯çš„è¯å½¢è¿˜åŸï¼Œä»¥åŒ¹é…è¾“å…¥æ–‡æœ¬
    """
    if not texts: return "N/A"
    
    # åˆå§‹åŒ–è¯å½¢è¿˜åŸå™¨
    lemmatizer = WordNetLemmatizer()
    
    # 1. å‡†å¤‡åŸºç¡€åœç”¨è¯
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    base_stop_words = list(ENGLISH_STOP_WORDS)
    
    # 2. åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰åœç”¨è¯
    combined_stopwords = base_stop_words
    if stop_words_list:
        combined_stopwords.extend(list(stop_words_list))
        
    # 3. â˜… å…³é”®æ­¥éª¤ï¼šå¯¹åœç”¨è¯è¡¨ä¹Ÿè¿›è¡Œè¯å½¢è¿˜åŸ
    # è¿™æ ·åœç”¨è¯è¡¨é‡Œå°±åŒæ—¶æœ‰äº† 'government' å’Œ 'government'ï¼ˆè¯å½¢è¿˜åŸç»“æœç›¸åŒï¼‰
    # æ— è®ºè¾“å…¥æ–‡æœ¬æ˜¯åŸå§‹çš„è¿˜æ˜¯è¯å½¢è¿˜åŸçš„ï¼Œéƒ½èƒ½è¢«è¿‡æ»¤
    lemmatized_stopwords = []
    for w in combined_stopwords:
        # ç®€å•è¯å½¢è¿˜åŸï¼ˆä¸ä½¿ç”¨POSæ ‡ç­¾ï¼Œå› ä¸ºåœç”¨è¯ä¸»è¦æ˜¯å¸¸è§è¯ï¼‰
        lemma = lemmatizer.lemmatize(w)
        lemmatized_stopwords.append(lemma)
    
    # åˆå¹¶ åŸè¯ + è¯å½¢è¿˜åŸè¯ (å»é‡)
    final_stop_words = list(set(combined_stopwords + lemmatized_stopwords))

    # 4. è¿è¡Œ TF-IDF
    tfidf = TfidfVectorizer(stop_words=final_stop_words, max_features=1000)
    
    try:
        tfidf_matrix = tfidf.fit_transform(texts)
        feature_names = np.array(tfidf.get_feature_names_out())
        avg_weights = np.array(tfidf_matrix.mean(axis=0)).flatten()
        top_indices = avg_weights.argsort()[::-1][:top_n]
        return ", ".join(feature_names[top_indices])
    except ValueError:
        return "N/A"

def get_average_vector(doc_tokens, w2v_model, vector_size):
    """
    è·å–æ–‡æ¡£å‘é‡ï¼šè®¡ç®—è¯å‘é‡çš„å¹³å‡å€¼
    """
    valid_tokens = [word for word in doc_tokens if word in w2v_model.wv.key_to_index]
    
    if not valid_tokens:
        return np.zeros(vector_size)
    
    # ç›´æ¥è®¡ç®—æœ‰æ•ˆè¯å‘é‡çš„å¹³å‡å€¼
    vec = np.mean([w2v_model.wv[word] for word in valid_tokens], axis=0)
    
    return vec

def find_optimal_k_elbow(doc_vectors, max_k=15, save_dir='clustering_analysis_8'):
    """è‚˜éƒ¨æ³•è‡ªåŠ¨å¯»æ‰¾æœ€ä½³K"""
    print(f"\n====== æ­£åœ¨è¿è¡Œè‚˜éƒ¨æ³• (Elbow Method) ======")
    wcss = []
    K_range = range(1, max_k + 1)
    
    for k in tqdm(K_range, desc="Calculating WCSS"):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(doc_vectors)
        wcss.append(kmeans.inertia_)
    
    x = np.array(K_range)
    y = np.array(wcss)
    x_norm = (x - x.min()) / (x.max() - x.min())
    y_norm = (y - y.min()) / (y.max() - y.min())
    
    start_point = np.array([x_norm[0], y_norm[0]])
    end_point = np.array([x_norm[-1], y_norm[-1]])
    line_vec = end_point - start_point
    
    distances = []
    for i in range(len(x)):
        point = np.array([x_norm[i], y_norm[i]])
        dist = np.abs(np.cross(line_vec, point - start_point)) / np.linalg.norm(line_vec)
        distances.append(dist)
    
    best_k = K_range[np.argmax(distances)]
    print(f"â˜… è‡ªåŠ¨æ£€æµ‹åˆ°çš„æœ€ä½³èšç±»æ•° (Elbow Point): K = {best_k}")

    plt.figure(figsize=(10, 6))
    plt.plot(K_range, wcss, 'bo-', label='WCSS (ç°‡å†…å¹³æ–¹å’Œ)')
    plt.plot(best_k, wcss[np.argmax(distances)], 'ro', markersize=12, label=f'æœ€ä½³Kå€¼={best_k}')
    plt.title('è‚˜éƒ¨æ³•ç¡®å®šæœ€ä½³èšç±»æ•° (Word2Vec)')
    plt.xlabel('èšç±»æ•° (k)')
    plt.ylabel('WCSS (ç°‡å†…å¹³æ–¹å’Œ)')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(save_dir, 'elbow_method_curve_tfidf.png'), dpi=300)
    
    return best_k

def main(k):
    start_total = time()

    # ==========================================
    # 1. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
    # ==========================================
    csv_path = 'corpus/ParlVote_concat.csv' 
    print(f"æ­£åœ¨è¯»å–æ•°æ®: {csv_path} ...")
    
    try:
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path, usecols=['motion_text', 'speech'])
        else:
            raise FileNotFoundError("CSV file not found")
    except Exception:
        print(f"è¯»å– CSV å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
        topics = ['economy', 'health', 'war', 'education', 'environment']
        data_text = [f"Motion about {topics[i%5]} and policy development" for i in range(2000)]
        df = pd.DataFrame({'motion_text': data_text, 'speech': data_text})

    all_texts = pd.concat([df['motion_text'], df['speech']]).dropna().astype(str)
    unique_texts = all_texts.unique()
    print(f"å”¯ä¸€æ–‡æ¡£æ•°é‡: {len(unique_texts)}")

    # åŠ è½½åœç”¨è¯ (å»ºè®®æŠŠ 'government', 'people', 'house' ç­‰é«˜é¢‘æ”¿æ²»è¯åŠ å…¥è¿™ä¸ªtxtæ–‡ä»¶)
    stopwords_set = load_stopwords('stopwords/w2v_stopwords.txt')

    # è®¾å®šæ˜¯å¦å¼€å¯è¯å½¢è¿˜åŸ
    USE_LEMMATIZATION = True 
    
    # å¹¶è¡Œå¤„ç†ï¼šå…ˆå»åœç”¨è¯ï¼Œå†è¯å½¢è¿˜åŸ
    tokenized_docs = Parallel(n_jobs=NUM_CORES)(
        delayed(preprocess_wrapper)(text, stopwords_set, USE_LEMMATIZATION) for text in tqdm(unique_texts, desc="Tokenizing")
    )
    
    valid_docs = []
    valid_indices = []
    for i, doc in enumerate(tokenized_docs):
        if len(doc) > 0:
            valid_docs.append(doc)
            valid_indices.append(i)
            
    unique_texts = unique_texts[valid_indices]
    print(f"æœ‰æ•ˆæ–‡æ¡£æ•°é‡: {len(valid_docs)}")

    # ==========================================
    # 3. Word2Vec æ¨¡å‹åŠ è½½æˆ–è®­ç»ƒ
    # ==========================================
    save_dir = f'clustering_analysis_{k}'
    os.makedirs(save_dir, exist_ok=True)
    # ä¿®æ”¹æ¨¡å‹æ–‡ä»¶åä»¥åŒºåˆ†æ˜¯å¦ä½¿ç”¨äº†è¯å¹²åŒ–
    model_name = 'word2vec_kmeans.model'
    model_path = os.path.join("models", model_name)
    
    model = None
    if os.path.exists(model_path):
        print(f"\nâ˜… æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹: {model_path}")
        try:
            loaded_model = Word2Vec.load(model_path)
            print(">>> æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            model = loaded_model
        except Exception as e:
            print(f">>> æ¨¡å‹åŠ è½½å‡ºé”™ ({e})ï¼Œå°†é‡æ–°è®­ç»ƒã€‚")
    
    if model is None:
        print(f"\næ­£åœ¨è®­ç»ƒæ–° Word2Vec æ¨¡å‹ (workers={NUM_CORES}, epochs=40)...")
        model = Word2Vec(sentences=valid_docs, vector_size=100, window=5, min_count=2, workers=NUM_CORES, epochs=40, seed=42, sg=1)
        model.save(model_path)
        print("æ¨¡å‹è®­ç»ƒå¹¶ä¿å­˜å®Œæ¯•ã€‚")

    # ==========================================
    # 4. ç”Ÿæˆæ–‡æ¡£å‘é‡ (ç›´æ¥è®¡ç®—è¯å‘é‡å¹³å‡å€¼)
    # ==========================================
    print("\nâ˜… æ­£åœ¨ç”Ÿæˆ Word2Vec å¹³å‡æ–‡æ¡£å‘é‡...")
    
    corpus_as_strings = [" ".join(doc) for doc in valid_docs]
    
    vector_size = model.vector_size
    doc_vectors = []
    
    for doc in tqdm(valid_docs, desc="Vectorizing"):
        vec = get_average_vector(doc, model, vector_size)
        doc_vectors.append(vec)
        
    doc_vectors = np.array(doc_vectors)
    doc_vectors_norm = normalize(doc_vectors)

    # ==========================================
    # 5. è‚˜éƒ¨æ³•ä¸èšç±»
    # ==========================================
    best_k = find_optimal_k_elbow(doc_vectors_norm, max_k=15, save_dir=save_dir)
    best_k = k

    print(f"æ­£åœ¨ä½¿ç”¨æœ€ä½³ K={best_k} è¿è¡Œæœ€ç»ˆèšç±»...")
    kmeans = KMeans(n_clusters=best_k, random_state=2026, n_init=10)
    kmeans_labels = kmeans.fit_predict(doc_vectors_norm)
    
    # è®¡ç®—èšç±»è¯„ä¼°æŒ‡æ ‡
    silhouette_avg = silhouette_score(doc_vectors_norm, kmeans_labels)
    ch_score = calinski_harabasz_score(doc_vectors_norm, kmeans_labels)
    dbi_score = davies_bouldin_score(doc_vectors_norm, kmeans_labels)
    
    print(f"\nğŸ” èšç±»è¯„ä¼°æŒ‡æ ‡:")
    print(f"   è½®å»“ç³»æ•° (Silhouette Score): {silhouette_avg:.4f}")
    print(f"   CHæŒ‡æ•° (Calinski-Harabasz Score): {ch_score:.4f}")
    print(f"   DBIæŒ‡æ•° (Davies-Bouldin Index): {dbi_score:.4f}")

    # ==========================================
    # 6. é™ç»´å¯è§†åŒ–
    # ==========================================
    print(f"æ­£åœ¨è¿›è¡Œ t-SNE é™ç»´...")
    tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto', n_jobs=NUM_CORES)
    vectors_2d = tsne.fit_transform(doc_vectors_norm)

    plot_df = pd.DataFrame({
        'x': vectors_2d[:, 0],
        'y': vectors_2d[:, 1],
        'KMeansèšç±»æ ‡ç­¾': kmeans_labels,
        'Raw_Text': unique_texts,
        'Processed_Text': corpus_as_strings  # ä¿å­˜ç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬
    })

    # ==========================================
    # 7. æå–å…³é”®è¯ä¸ç»˜å›¾
    # ==========================================
    print("\nâ˜… æ­£åœ¨åˆ†æèšç±»å…³é”®è¯...")
    cluster_keywords = {}
    stopwords_list = list(stopwords_set) if stopwords_set else []

    for c in range(best_k):
        # ä½¿ç”¨ç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬æå–å…³é”®è¯
        texts = plot_df[plot_df['KMeansèšç±»æ ‡ç­¾'] == c]['Processed_Text'].tolist()
        keywords = get_cluster_keywords(texts, stop_words_list=stopwords_list)
        cluster_keywords[c] = keywords
        print(f"  C{c}: [{keywords}]")

    print("\næ­£åœ¨ç»˜å›¾...")
    plt.figure(figsize=(16, 12)) 
    sns.scatterplot(data=plot_df, x='x', y='y', hue='KMeansèšç±»æ ‡ç­¾', palette='viridis', s=40, alpha=0.6)
    
    for c in range(best_k):
        cluster_points = plot_df[plot_df['KMeansèšç±»æ ‡ç­¾'] == c]
        if len(cluster_points) == 0: continue
        label_text = f"ç°‡{c}\n{cluster_keywords.get(c, '')}"
        plt.text(cluster_points['x'].mean(), cluster_points['y'].mean(), label_text, 
                 ha='center', va='center', fontsize=11, weight='bold', 
                 bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))

    plt.title(f'Word2Vec + K-Means èšç±»å¯è§†åŒ–', fontsize=18)
    save_filename = 'word2vec_lemmatized_figure.png' if USE_LEMMATIZATION else 'word2vec_figure.png'
    plt.savefig(os.path.join(save_dir, save_filename), dpi=300)
    
    # å°†èšç±»ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶
    csv_output_path = os.path.join(save_dir, 'clustering_results.csv')
    # é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—ï¼šèšç±»æ ‡ç­¾å’ŒåŸå§‹æ–‡æœ¬
    output_df = plot_df[['KMeansèšç±»æ ‡ç­¾', 'Raw_Text']]
    output_df.to_csv(csv_output_path, index=False, encoding='utf-8')
    print(f"èšç±»ç»“æœå·²ä¿å­˜åˆ°: {csv_output_path}")
    
    print(f"ç»“æœå·²ä¿å­˜ã€‚æ€»è€—æ—¶: {time() - start_total:.2f} ç§’")

if __name__ == '__main__':
    multiprocessing.freeze_support()
    main(10)