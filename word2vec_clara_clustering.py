import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import multiprocessing
import os
from time import time
from collections import Counter

# NLP & æœºå™¨å­¦ä¹ åº“
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from sklearn.manifold import TSNE
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

# å¹¶è¡Œå¤„ç†åº“
from joblib import Parallel, delayed
from tqdm import tqdm

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_context("notebook", font_scale=1.2)
sns.set_style("whitegrid")
plt.rcParams['axes.unicode_minus'] = False
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
except:
    pass

# ==========================================
# 0. è®¾å®šæ ¸å¿ƒæ•°
# ==========================================
NUM_CORES = max(1, multiprocessing.cpu_count() - 2)
print(f"â˜… å·²è®¾å®šä½¿ç”¨ {NUM_CORES} ä¸ªçº¿ç¨‹è¿›è¡Œå¹¶è¡ŒåŠ é€Ÿ")

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def get_wordnet_pos(tag):
    """
    å°†NLTKçš„POSæ ‡ç­¾è½¬æ¢ä¸ºWordNetçš„POSæ ‡ç­¾
    """
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # é»˜è®¤ä½¿ç”¨åè¯

def load_stopwords(filepath='stopwords/w2v_stopwords_clara.txt'):
    stopwords = set()
    if os.path.exists(filepath):
        print(f"æ­£åœ¨åŠ è½½åœç”¨è¯è¡¨: {filepath} ...")
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                stopwords = set(line.strip() for line in f if line.strip())
            print(f"æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯ã€‚")
        except Exception as e:
            print(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
    return stopwords

# â˜… ä¿®æ”¹ï¼šæ·»åŠ  use_lemmatization å‚æ•°å¹¶å®ç°è¯å½¢è¿˜åŸé€»è¾‘
def preprocess_wrapper(text, stopwords_set=None, use_lemmatization=True):
    """
    é¢„å¤„ç†æµç¨‹ï¼šåˆ†è¯ -> å»åœç”¨è¯ -> è¯å½¢è¿˜åŸ
    """
    if pd.isna(text):
        return []
    
    # 1. åˆ†è¯ (Gensim çš„ simple_preprocess ä¼šè‡ªåŠ¨è½¬å°å†™å¹¶å»æ ‡ç‚¹)
    tokens = simple_preprocess(str(text))
    
    # 2. å…ˆå»åœç”¨è¯ (æ­¤æ—¶ token æ˜¯å®Œæ•´çš„ï¼Œèƒ½åŒ¹é…ä¸Š stopwords é‡Œçš„ 'government')
    if stopwords_set:
        tokens = [t for t in tokens if t not in stopwords_set]
    
    # 3. æœ€åå¯¹å‰©ä¸‹çš„è¯è¿›è¡Œè¯å½¢è¿˜åŸ
    if use_lemmatization:
        # åœ¨å‡½æ•°å†…åˆå§‹åŒ–ï¼Œé˜²æ­¢å¤šè¿›ç¨‹å†²çª
        lemmatizer = WordNetLemmatizer()
        # ç®€åŒ–è¯å½¢è¿˜åŸï¼ˆä¸ä½¿ç”¨POSæ ‡ç­¾ï¼Œé¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜ï¼‰
        tokens = [lemmatizer.lemmatize(t) for t in tokens]
        
    return tokens

def get_cluster_keywords(texts, stop_words_list=None, top_n=5):
    """
    æå–èšç±»å…³é”®è¯ (TF-IDF)
    è‡ªåŠ¨å¤„ç†åœç”¨è¯çš„è¯å½¢è¿˜åŸï¼Œä»¥åŒ¹é…è¾“å…¥æ–‡æœ¬
    """
    if not texts: return "N/A"
    
    # åˆå§‹åŒ–è¯å½¢è¿˜åŸå™¨
    lemmatizer = WordNetLemmatizer()
    
    # 1. å‡†å¤‡åŸºç¡€åœç”¨è¯
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    base_stop_words = list(ENGLISH_STOP_WORDS)
    
    # 2. åˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰åœç”¨è¯
    combined_stopwords = base_stop_words
    if stop_words_list:
        combined_stopwords.extend(list(stop_words_list))
        
    # 3. â˜… å…³é”®æ­¥éª¤ï¼šå¯¹åœç”¨è¯è¡¨ä¹Ÿè¿›è¡Œè¯å½¢è¿˜åŸ
    # è¿™æ ·åœç”¨è¯è¡¨é‡Œå°±åŒæ—¶æœ‰äº† 'government' å’Œ 'government'ï¼ˆè¯å½¢è¿˜åŸç»“æœç›¸åŒï¼‰
    # æ— è®ºè¾“å…¥æ–‡æœ¬æ˜¯åŸå§‹çš„è¿˜æ˜¯è¯å½¢è¿˜åŸçš„ï¼Œéƒ½èƒ½è¢«è¿‡æ»¤
    lemmatized_stopwords = []
    for w in combined_stopwords:
        # ç®€å•è¯å½¢è¿˜åŸï¼ˆä¸ä½¿ç”¨POSæ ‡ç­¾ï¼Œå› ä¸ºåœç”¨è¯ä¸»è¦æ˜¯å¸¸è§è¯ï¼‰
        lemma = lemmatizer.lemmatize(w)
        lemmatized_stopwords.append(lemma)
    
    # åˆå¹¶ åŸè¯ + è¯å½¢è¿˜åŸè¯ (å»é‡)
    final_stop_words = list(set(combined_stopwords + lemmatized_stopwords))

    # 4. è¿è¡Œ TF-IDF
    tfidf = TfidfVectorizer(stop_words=final_stop_words, max_features=1000)
    
    try:
        tfidf_matrix = tfidf.fit_transform(texts)
        feature_names = np.array(tfidf.get_feature_names_out())
        avg_weights = np.array(tfidf_matrix.mean(axis=0)).flatten()
        top_indices = avg_weights.argsort()[::-1][:top_n]
        return ", ".join(feature_names[top_indices])
    except ValueError:
        return "N/A"

def get_tfidf_weighted_vector(doc_tokens, w2v_model, idf_dict, vector_size):
    """
    è·å–æ–‡æ¡£å‘é‡ï¼šè®¡ç®— TF-IDF åŠ æƒå¹³å‡å€¼
    """
    valid_tokens = [word for word in doc_tokens if word in w2v_model.wv.key_to_index and word in idf_dict]
    
    if not valid_tokens:
        return np.zeros(vector_size)
    
    tf_counter = Counter(valid_tokens)
    total_tokens = len(valid_tokens)
    
    weighted_sum = np.zeros(vector_size)
    total_weight = 0.0
    
    for word, count in tf_counter.items():
        vec = w2v_model.wv[word]
        tf = count / total_tokens
        idf = idf_dict[word]
        weight = tf * idf
        
        weighted_sum += vec * weight
        total_weight += weight
        
    if total_weight == 0:
        return np.zeros(vector_size)
        
    return weighted_sum / total_weight


def main():
    start_total = time()

    # ==========================================
    # 1. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
    # ==========================================
    csv_path = 'corpus/ParlVote_concat.csv' 
    print(f"æ­£åœ¨è¯»å–æ•°æ®: {csv_path} ...")
    
    try:
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path, usecols=['motion_text', 'speech'])
        else:
            raise FileNotFoundError("CSV file not found")
    except Exception:
        print(f"è¯»å– CSV å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®...")
        topics = ['economy', 'health', 'war', 'education', 'environment']
        data_text = [f"Motion about {topics[i%5]} and policy development" for i in range(2000)]
        df = pd.DataFrame({'motion_text': data_text, 'speech': data_text})

    all_texts = pd.concat([df['motion_text'], df['speech']]).dropna().astype(str)
    unique_texts = all_texts.unique()
    print(f"å”¯ä¸€æ–‡æ¡£æ•°é‡: {len(unique_texts)}")

    # åŠ è½½åœç”¨è¯ (å»ºè®®æŠŠ 'government', 'people', 'house' ç­‰é«˜é¢‘æ”¿æ²»è¯åŠ å…¥è¿™ä¸ªtxtæ–‡ä»¶)
    stopwords_set = load_stopwords('stopwords/w2v_stopwords.txt')

    # è®¾å®šæ˜¯å¦å¼€å¯è¯å½¢è¿˜åŸ
    USE_LEMMATIZATION = True 
    
    # å¹¶è¡Œå¤„ç†ï¼šå…ˆå»åœç”¨è¯ï¼Œå†è¯å½¢è¿˜åŸ
    tokenized_docs = Parallel(n_jobs=NUM_CORES)(
        delayed(preprocess_wrapper)(text, stopwords_set, USE_LEMMATIZATION) for text in tqdm(unique_texts, desc="Tokenizing")
    )
    
    valid_docs = []
    valid_indices = []
    for i, doc in enumerate(tokenized_docs):
        if len(doc) > 0:
            valid_docs.append(doc)
            valid_indices.append(i)
            
    unique_texts = unique_texts[valid_indices]
    print(f"æœ‰æ•ˆæ–‡æ¡£æ•°é‡: {len(valid_docs)}")

    # ==========================================
    # 3. Word2Vec æ¨¡å‹åŠ è½½æˆ–è®­ç»ƒ
    # ==========================================
    save_dir = 'clustering_analysis_CLARA'
    os.makedirs(save_dir, exist_ok=True)
    # ä¿®æ”¹æ¨¡å‹æ–‡ä»¶åä»¥åŒºåˆ†æ˜¯å¦ä½¿ç”¨äº†è¯å¹²åŒ–
    model_name = 'word2vec_clara.model'
    model_path = os.path.join("models", model_name)
    
    model = None
    if os.path.exists(model_path):
        print(f"\nâ˜… æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹: {model_path}")
        try:
            loaded_model = Word2Vec.load(model_path)
            print(">>> æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            model = loaded_model
        except Exception as e:
            print(f">>> æ¨¡å‹åŠ è½½å‡ºé”™ ({e})ï¼Œå°†é‡æ–°è®­ç»ƒã€‚")
    
    if model is None:
        print(f"\næ­£åœ¨è®­ç»ƒæ–° Word2Vec æ¨¡å‹ (workers={NUM_CORES}, epochs=40)...")
        model = Word2Vec(sentences=valid_docs, vector_size=100, window=5, min_count=2, workers=NUM_CORES, epochs=40, seed=42, sg=1)
        model.save(model_path)
        print("æ¨¡å‹è®­ç»ƒå¹¶ä¿å­˜å®Œæ¯•ã€‚")

    # ==========================================
    # 4. ç”Ÿæˆæ–‡æ¡£å‘é‡ (ç›´æ¥ä½¿ç”¨Word2Vecå¹³å‡å‘é‡)
    # ==========================================
    print("\nâ˜… æ­£åœ¨ç”Ÿæˆ Word2Vec å¹³å‡æ–‡æ¡£å‘é‡...")
    
    corpus_as_strings = [" ".join(doc) for doc in valid_docs]
    
    vector_size = model.vector_size
    doc_vectors = []
    
    for doc in tqdm(valid_docs, desc="Vectorizing"):
        # ç›´æ¥è®¡ç®—æ–‡æ¡£ä¸­æ‰€æœ‰è¯å‘é‡çš„å¹³å‡å€¼
        valid_words = [word for word in doc if word in model.wv.key_to_index]
        if valid_words:
            vec = np.mean([model.wv[word] for word in valid_words], axis=0)
        else:
            vec = np.zeros(vector_size)
        doc_vectors.append(vec)
        
    doc_vectors = np.array(doc_vectors)
    doc_vectors_norm = normalize(doc_vectors)

    # ==========================================
    # 5. CLARA èšç±»
    # ==========================================
    print(f"\næ­£åœ¨ä½¿ç”¨ CLARA è¿è¡Œèšç±»...")
    
    # è®¾ç½®CLARAå‚æ•°
    num_clusters = 10  # è®¾å®šèšç±»æ•°
    samples = 5  # æ ·æœ¬æ•°
    sample_size = 1000  # æ¯ä¸ªæ ·æœ¬çš„å¤§å°
    random_seed = 420  # éšæœºç§å­ï¼Œç”¨äºç¡®ä¿ç»“æœå¯å¤ç°
    
    # è®¾ç½®numpyéšæœºç§å­
    np.random.seed(random_seed)
    
    # å®šä¹‰ä½™å¼¦è·ç¦»è®¡ç®—å‡½æ•°
    def cosine_distance(a, b):
        return 1 - np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    # ç®€åŒ–ç‰ˆKMedoidså®ç°
    def simple_kmedoids(vectors, k, seed=random_seed):
        # éšæœºé€‰æ‹©åˆå§‹ä¸­å¿ƒç‚¹
        n = len(vectors)
        rng = np.random.default_rng(seed)
        medoid_indices = rng.choice(n, k, replace=False)
        medoids = vectors[medoid_indices]
        
        # è¿­ä»£ä¼˜åŒ–
        max_iter = 100
        for _ in range(max_iter):
            # åˆ†é…æ¯ä¸ªç‚¹åˆ°æœ€è¿‘çš„ä¸­å¿ƒç‚¹
            labels = np.zeros(n, dtype=int)
            for i in range(n):
                distances = [cosine_distance(vectors[i], m) for m in medoids]
                labels[i] = np.argmin(distances)
            
            # æ›´æ–°ä¸­å¿ƒç‚¹
            new_medoid_indices = np.zeros(k, dtype=int)
            for i in range(k):
                cluster_points = vectors[labels == i]
                if len(cluster_points) == 0:
                    continue
                
                # è®¡ç®—ç°‡å†…æ¯ä¸ªç‚¹ä½œä¸ºä¸­å¿ƒç‚¹çš„æˆæœ¬
                min_cost = float('inf')
                best_medoid = 0
                
                for j, point in enumerate(cluster_points):
                    cost = sum([cosine_distance(point, p) for p in cluster_points])
                    if cost < min_cost:
                        min_cost = cost
                        best_medoid = j
                
                new_medoid_indices[i] = np.where((vectors == cluster_points[best_medoid]).all(axis=1))[0][0]
            
            # æ£€æŸ¥æ˜¯å¦æ”¶æ•›
            if np.array_equal(medoid_indices, new_medoid_indices):
                break
            
            medoid_indices = new_medoid_indices
            medoids = vectors[medoid_indices]
        
        # è®¡ç®—æœ€ç»ˆæˆæœ¬
        cost = 0
        for i in range(n):
            distances = [cosine_distance(vectors[i], m) for m in medoids]
            cost += min(distances)
        
        return medoid_indices, labels, cost
    
    print(f"æ­£åœ¨è¿è¡Œç®€åŒ–ç‰ˆ CLARA ç®—æ³•: {samples} ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å¤§å° {sample_size}...")
    
    best_cost = float('inf')
    best_medoids = None
    best_labels = None
    
    for i in tqdm(range(samples), desc="CLARA Samples"):
        # 1. éšæœºæŠ½å–æ ·æœ¬
        sample_indices = np.random.choice(len(doc_vectors_norm), sample_size, replace=False)
        sample_vectors = doc_vectors_norm[sample_indices]
        
        # 2. åœ¨æ ·æœ¬ä¸Šè¿è¡Œç®€åŒ–ç‰ˆKMedoidsï¼Œä¼ é€’ä¸åŒçš„ç§å­ä»¥å¢åŠ å¤šæ ·æ€§
        medoid_indices_in_sample, _, cost = simple_kmedoids(sample_vectors, num_clusters, seed=random_seed + i)
        
        # 3. ä¿å­˜æœ€ä½³ç»“æœ
        if cost < best_cost:
            best_cost = cost
            
            # è·å–æ ·æœ¬ä¸­çš„ä¸­å¿ƒç‚¹åœ¨åŸå§‹æ•°æ®ä¸­çš„ç´¢å¼•
            best_medoids = sample_indices[medoid_indices_in_sample]
    
    # 4. ä½¿ç”¨æœ€ä½³ä¸­å¿ƒç‚¹å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œèšç±»
    print("æ­£åœ¨ä½¿ç”¨æœ€ä½³ä¸­å¿ƒç‚¹å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œèšç±»...")
    clara_labels = np.zeros(len(doc_vectors_norm), dtype=int)
    
    for i in tqdm(range(len(doc_vectors_norm)), desc="Assigning labels"):
        distances = [cosine_distance(doc_vectors_norm[i], doc_vectors_norm[m]) for m in best_medoids]
        clara_labels[i] = np.argmin(distances)
    
    # ç»Ÿè®¡èšç±»ç»“æœ
    cluster_counts = Counter(clara_labels)
    print(f"CLARA èšç±»ç»“æœç»Ÿè®¡:")
    for cluster_id, count in sorted(cluster_counts.items()):
        print(f"  ç°‡ {cluster_id}: {count} ä¸ªæ ·æœ¬")
    
    total_clusters = len(set(clara_labels))
    print(f"æ€»èšç±»æ•°: {total_clusters} ä¸ª")
    
    # è®¡ç®—èšç±»è¯„ä¼°æŒ‡æ ‡
    silhouette_avg = silhouette_score(doc_vectors_norm, clara_labels)
    ch_score = calinski_harabasz_score(doc_vectors_norm, clara_labels)
    dbi_score = davies_bouldin_score(doc_vectors_norm, clara_labels)
    
    print(f"\nğŸ” èšç±»è¯„ä¼°æŒ‡æ ‡:")
    print(f"   è½®å»“ç³»æ•° (Silhouette Score): {silhouette_avg:.4f}")
    print(f"   CHæŒ‡æ•° (Calinski-Harabasz Score): {ch_score:.4f}")
    print(f"   DBIæŒ‡æ•° (Davies-Bouldin Index): {dbi_score:.4f}")

    # ==========================================
    # 6. é™ç»´å¯è§†åŒ–
    # ==========================================
    print(f"æ­£åœ¨è¿›è¡Œ t-SNE é™ç»´...")
    tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto', n_jobs=NUM_CORES)
    vectors_2d = tsne.fit_transform(doc_vectors_norm)

    plot_df = pd.DataFrame({
        'x': vectors_2d[:, 0],
        'y': vectors_2d[:, 1],
        'CLARAèšç±»æ ‡ç­¾': clara_labels,
        'Raw_Text': unique_texts,
        'Processed_Text': corpus_as_strings  # ä¿å­˜ç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬
    })

    # ==========================================
    # 7. æå–å…³é”®è¯ä¸ç»˜å›¾
    # ==========================================
    print("\nâ˜… æ­£åœ¨åˆ†æèšç±»å…³é”®è¯...")
    cluster_keywords = {}
    stopwords_list = list(stopwords_set) if stopwords_set else []

    # è·å–æ‰€æœ‰å”¯ä¸€çš„èšç±»æ ‡ç­¾
    unique_clusters = sorted(set(clara_labels))
    for c in unique_clusters:
        # ä½¿ç”¨ç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬æå–å…³é”®è¯
        texts = plot_df[plot_df['CLARAèšç±»æ ‡ç­¾'] == c]['Processed_Text'].tolist()
        keywords = get_cluster_keywords(texts, stop_words_list=stopwords_list)
        cluster_keywords[c] = keywords
        print(f"  ç°‡ {c}: [{keywords}]")

    print("\næ­£åœ¨ç»˜å›¾...")
    plt.figure(figsize=(16, 12)) 
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    sns.scatterplot(data=plot_df, x='x', y='y', hue='CLARAèšç±»æ ‡ç­¾', palette='viridis', s=40, alpha=0.6)
    
    # æ·»åŠ èšç±»ä¸­å¿ƒæ ‡ç­¾å’Œå…³é”®è¯
    for c in unique_clusters:
        cluster_points = plot_df[plot_df['CLARAèšç±»æ ‡ç­¾'] == c]
        if len(cluster_points) == 0: continue
        label_text = f"ç°‡{c}\n{cluster_keywords.get(c, '')}"
        plt.text(cluster_points['x'].mean(), cluster_points['y'].mean(), label_text, 
                 ha='center', va='center', fontsize=11, weight='bold', 
                 bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))

    plt.title(f'Word2Vec + CLARA èšç±»å¯è§†åŒ–', fontsize=18)
    plt.savefig(os.path.join(save_dir, 'word2vec_lemmatized_clara_figure.png'), dpi=300)
    
    # å°†èšç±»ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶
    csv_output_path = os.path.join(save_dir, 'clustering_results.csv')
    # é€‰æ‹©éœ€è¦ä¿å­˜çš„åˆ—ï¼šèšç±»æ ‡ç­¾å’ŒåŸå§‹æ–‡æœ¬
    output_df = plot_df[['CLARAèšç±»æ ‡ç­¾', 'Raw_Text']]
    output_df.to_csv(csv_output_path, index=False, encoding='utf-8')
    print(f"èšç±»ç»“æœå·²ä¿å­˜åˆ°: {csv_output_path}")
    
    print(f"ç»“æœå·²ä¿å­˜ã€‚æ€»è€—æ—¶: {time() - start_total:.2f} ç§’")

if __name__ == '__main__':
    multiprocessing.freeze_support()
    main()